% This file was created with JabRef 2.8.1.
% Encoding: Cp1252

@INPROCEEDINGS{Sislak:2008:AMA:1402744.1402755,
  author = {\v{S}i\v{s}l\'{a}k, David and Volf, P\v{r}emysl and Kop\v{r}iva,
	\v{S}t\v{e}p\'{a}n and P\v{e}chou\v{c}ek, Michal},
  title = {AGENTFLY: a multi-agent airspace test-bed},
  booktitle = {Proceedings of the 7th international joint conference on Autonomous
	agents and multiagent systems: demo papers},
  year = {2008},
  series = {AAMAS '08},
  pages = {1665--1666},
  address = {Richland, SC},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  acmid = {1402755},
  keywords = {agent-based collision avoidance, autonomous aircrafts, multi-agent
	simulation},
  location = {Estoril, Portugal},
  numpages = {2},
  url = {http://dl.acm.org/citation.cfm?id=1402744.1402755}
}

@INPROCEEDINGS{AAMAS12-agmon,
  author = {Noa Agmon and Peter Stone},
  title = {Leading Ad Hoc Agents in Joint Action Settings with Multiple Teammates},
  booktitle = {Proc. of 11th Int. Conf. on Autonomous Agents and Multiagent Systems
	(AAMAS 2012)},
  year = {2012},
  month = {June},
  abstract = { The growing use of autonomous agents in practice may require agents
	to cooperate as a team in situations where they have limited prior
	knowledge about one another, cannot communicate directly, or do not
	share the same world models. These situations raise the need to design
	ad hoc team members, i.e., agents that will be able to cooperate
	without coordination in order to reach an optimal team behavior.
	This paper considers the problem of leading N-agent teams by an agent
	toward their optimal joint utility, where the agents compute their
	next actions based only on their most recent observations of their
	teammatesâ€™ actions. We show that compared to previous results in
	two-agent teams, in larger teams the agent might not be able to lead
	the team to the action with maximal joint utility, thus its optimal
	strategy is to lead the team to the best possible reachable cycle
	of joint actions. We describe a graphical model of the problem and
	a polynomial time algorithm for solving it. We then consider other
	variations of the problem, including leading teams of agents where
	they base their actions on longer history of past observations, leading
	a team by more than one ad hoc agent, and leading a teammate while
	the ad hoc agent is uncertain of its behavior. },
  location = {Valencia, Spain}
}

@INPROCEEDINGS{AdrianNotUsed,
  author = {Agogino, A. and Rios, J.},
  title = {Robustness of two air traffic scheduling approaches to departure
	uncertainty},
  booktitle = {Proceedings of the Digital Avionics Systems Conference (DASC), 2011
	IEEE/AIAA 30th},
  year = {2011},
  pages = {2C6-1 -2C6-8},
  month = {oct.},
  doi = {10.1109/DASC.2011.6095996},
  issn = {2155-7195},
  keywords = {air traffic scheduling;binary programming approach;departure uncertainty;fast-learning
	evolutionary algorithm;large scale air traffic flow problem;linear
	programming method;nonlinear evolutionary algorithm-based optimization
	technique;air traffic control;evolutionary computation;linear programming;nonlinear
	programming;robust control;uncertain systems;}
}

@ARTICLE{tumer-agogino_jaamas12,
  author = {A. K. Agogino and K. Tumer},
  title = {A Multiagent Approach to Managing Air Traffic Flow},
  journal = {Autonomous Agents and MultiAgent Systems},
  year = {2012},
  volume = {24},
  pages = {1-25},
  note = {(DOI: 10.1007/s10458-010-9142-5)},
  abstract = {Intelligent air traffic flow management is one of the fundamental
	challenges facing the Federal Aviation Administration (FAA) today.
	FAA estimates put weather, routing decisions and airport condition
	induced delays at 1,682,700 hours in 2007, resulting in a staggering
	economic loss of over $41 Billion. New solutions to the flow management
	are needed to accommodate the threefold increase in air traffic anticipated
	over the next two decades. Indeed, this is a complex problem where
	the interactions of changing conditions (e.g., weather), conflicting
	priorities (e.g., different airlines), limited resources (e.g., air
	traffic controllers) and heavy volume (e.g., over 40,000 flights
	over the US airspace) demand an adaptive and robust solution. In
	this paper we explore a multiagent algorithm where agents use reinforcement
	learning to reduce congestion through local actions. Each agent is
	associated with a fix (a specific location in 2D space) and has one
	of three actions: setting separation between airplanes, ordering
	ground delays or performing reroutes. We simulate air traffic using
	FACET which is an air traffic flow simulator developed at NASA and
	used extensively by the FAA and industry. Our FACET simulations on
	both artificial and real historical data from the Chicago and New
	York airspaces show that agents receiving personalized rewards reduce
	congestion by up to 80\% over agents receiving a global reward and
	by up to 90\% over a current industry approach (Monte Carlo estimation).},
  bib2html_pubtype = {Journal Articles},
  bib2html_rescat = {Air Traffic Control, Multiagent Systems, Traffic and Transportation}
}

@ARTICLE{tumer-agogino_jaamas08,
  author = {A. K. Agogino and K. Tumer},
  title = {Analyzing and Visualizing Multiagent Rewards in Dynamic and Stochastic
	Environments},
  journal = {Journal of Autonomous Agents and Multi-Agent Systems},
  year = {2008},
  volume = {17},
  pages = {320-338},
  number = {2},
  abstract = { The ability to analyze the effectiveness of agent reward structures
	is critical to the successful design of multiagent learning algorithms.
	Though final system performance is the best indicator of the suitability
	of a given reward structure, it is often preferable to analyze the
	reward properties that lead to good system behavior (i.e., properties
	promoting coordination among the agents and providing agents with
	strong signal to noise ratios). This step is particularly helpful
	in continuous, dynamic, stochastic domains ill-suited to simple table
	backup schemes commonly used in TD(\lambda)/Q-learning where the
	effectiveness of the reward structure is difficult to distinguish
	from the effectiveness of the chosen learning algorithm. In this
	paper, we present a new reward evaluation method that provides a
	visualization of the tradeoff between the level of coordination among
	the agents and the difficulty of the learning problem each agent
	faces. This method is independent of the learning algorithm and is
	only a function of the problem domain and the agents' reward structure.
	We use this reward property visualization method to determine an
	effective reward without performing extensive simulations. We then
	test this method in both a static and a dynamic multi-rover learning
	domain where the agents have continuous state spaces and take noisy
	actions (e.g., the agents' movement decisions are not always carried
	out properly). Our results show that in the more difficult dynamic
	domain, the reward efficiency visualization method provides a two
	order of magnitude speedup in selecting good rewards, compared to
	running a full simulation. In addition, this method facilitates the
	design and analysis of new rewards tailored to the observational
	limitations of the domain, providing rewards that combine the best
	properties of traditional rewards.},
  bib2html_pubtype = {Journal Articles},
  bib2html_rescat = {Reinforcement Learning, Multiagent Systems}
}

@CONFERENCE{Bertsimas,
  author = {Bertsimas, D. and Patterson, S.S.},
  title = {The Air Traffic Flow Management Problem with Enroute Capacities},
  booktitle = {May-June 1998, pp. 406–422},
  year = {1998},
  owner = {Will},
  timestamp = {2012.10.09}
}

@ARTICLE{FACET,
  author = {Bilimoria, K. D., B. Sridhar, G. B. Chatterji, K. S. Shethand, and
	S. R. Grabbe},
  title = {FACET: Future ATM Concepts Evaluation Tool},
  journal = {Air Traffc Control Quarterly},
  year = {2001},
  volume = {9},
  pages = {1},
  owner = {Will},
  timestamp = {2012.10.12}
}

@CONFERENCE{Agglomerative,
  author = {William H.E. Day and Herbert Edelsbrunner},
  title = {Efficient Algorithms for Agglomerative Hierarchical Clustering Methods},
  booktitle = {Journal of Classification},
  year = {1984},
  number = {1},
  series = {7-24},
  owner = {Will},
  timestamp = {2012.10.12}
}

@ARTICLE{Hardin,
  author = {G. Hardin},
  title = {The tragedy of the commons},
  journal = {Science},
  year = {December 1968},
  volume = {162},
  pages = {1243–1248},
  owner = {Will},
  timestamp = {2012.10.09}
}

@UNPUBLISHED{faa05,
  author = {FAA OPSNET data Jan-Dec 2011},
  title = {{US} {D}epartment of {T}ransportation website},
  note = {(http://www.faa.gov/data\_statistics/)},
  year = {2011}
}

@MISC{Nobody06,
  author = {Nobody Jr},
  title = {My Article},
  year = {2006}
}

@INPROCEEDINGS{Junges:2008:EPD:1402298.1402308,
  author = {Junges, Robert and Bazzan, Ana L. C.},
  title = {Evaluating the performance of DCOP algorithms in a real world, dynamic
	problem},
  booktitle = {Proceedings of the 7th international joint conference on Autonomous
	agents and multiagent systems - Volume 2},
  year = {2008},
  series = {AAMAS '08},
  pages = {599--606},
  address = {Richland, SC},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  acmid = {1402308},
  isbn = {978-0-9817381-1-6},
  keywords = {coordination, distributed constraint optimization, traffic control},
  location = {Estoril, Portugal},
  numpages = {8},
  url = {http://dl.acm.org/citation.cfm?id=1402298.1402308}
}

@INPROCEEDINGS{5509316,
  author = {Kaminka, G.A. and Erusalimchik, D. and Kraus, S.},
  title = {Adaptive multi-robot coordination: A game-theoretic perspective},
  booktitle = {Robotics and Automation (ICRA), 2010 IEEE International Conference
	on},
  year = {2010},
  pages = {328 -334},
  month = {may},
  doi = {10.1109/ROBOT.2010.5509316},
  issn = {1050-4729},
  keywords = {adaptive multirobot coordination;coordination algorithm selection;effectiveness
	index;game-theoretic perspective;matrix games;multirobot foraging;multirobot
	learning;reinforcement-learning approach;resource-spending velocity;reward
	function;spatial coordination;game theory;learning systems;multi-robot
	systems;robot dynamics;}
}

@ARTICLE{tumer-knudson_acs12NotUsed,
  author = {M. Knudson and K. Tumer},
  title = {Dynamic Partnership Formation for Multi-Rover Coordination},
  journal = {Advances in Complex Systems},
  year = {2012},
  note = {(to appear)},
  abstract = {Coordinating multiagent systems to maximize global information collection
	is a key challenge in many real world applications such as planetary
	exploration, and search and rescue. In particular, in many domains
	where communication is expensive (e.g. in terms of energy), the coordination
	must be achieved in a passive manner, without agents explicitly informing
	other agents of their states and/or intended actions. In this work,
	we extend results on such multiagent coordination algorithms to domains
	where the agents cannot achieve the required tasks without forming
	teams. We investigate team formation in three types of domains, one
	where n agents need to perform a task for the team to receive credit,
	one where there is an optimal number of agents (n) required for the
	task, but where the agents receive a decaying reward if they form
	a team with membership other than n, and finally we investigate heterogeneous
	teams where individuals vary in construction. Our results show that
	encouraging agents to coordinate is more successful than strictly
	requiring coordination. We also show that devising agent objective
	functions that are aligned with the global objective and locally
	computable significantly outperform systems where agents directly
	use the global objective, and that the improvement increases with
	the complexity of the task.},
  bib2html_pubtype = {Journal Articles},
  bib2html_rescat = {Multiagent Systems, Robotics}
}

@ARTICLE{Kok:2006:CMR:1248547.1248612,
  author = {Kok, Jelle R. and Vlassis, Nikos},
  title = {Collaborative Multiagent Reinforcement Learning by Payoff Propagation},
  journal = {J. Mach. Learn. Res.},
  year = {2006},
  volume = {7},
  pages = {1789--1828},
  month = dec,
  acmid = {1248612},
  issn = {1532-4435},
  issue_date = {12/1/2006},
  numpages = {40},
  publisher = {JMLR.org},
  url = {http://dl.acm.org/citation.cfm?id=1248547.1248612}
}

@INPROCEEDINGS{Panait,
  author = {Liviu Panait, Keith Sullivan, and Sean Luke.},
  title = {Lenience towards teammates helps in cooperative multiagent learning.},
  booktitle = {Proceedings of the Fifth International Joint Conference on Autonomous
	Agents and Multi Agent Systems},
  year = {2006},
  owner = {Will},
  timestamp = {2012.10.10}
}

@ARTICLE{Modi:2005:AAD:1120120.1120127,
  author = {Modi, Pragnesh Jay and Shen, Wei-Min and Tambe, Milind and Yokoo,
	Makoto},
  title = {Adopt: asynchronous distributed constraint optimization with quality
	guarantees},
  journal = {Artif. Intell.},
  year = {2005},
  volume = {161},
  pages = {149--180},
  number = {1-2},
  month = jan,
  acmid = {1120127},
  address = {Essex, UK},
  doi = {10.1016/j.artint.2004.09.003},
  issn = {0004-3702},
  issue_date = {January 2005},
  keywords = {constraints, distributed optimization, multiagent systems},
  numpages = {32},
  publisher = {Elsevier Science Publishers Ltd.},
  url = {http://dx.doi.org/10.1016/j.artint.2004.09.003}
}

@INPROCEEDINGS{Rios,
  author = {Rios, J. and Lohn, J.},
  title = {A Comparison of Optimization Approaches for Nationwide Traffic Flow
	Management},
  booktitle = {Proceedings of the AIAA Guidance, Navigation, and Control Conference,
	Chicago, Illinois},
  year = {August 2009},
  owner = {Will},
  timestamp = {2012.10.09}
}

@ARTICLE{Sridhar,
  author = {Sridhar, B. and Grabbe, S.R. and Mukherjee, A.},
  title = {Modeling and Optimization in Traffic Flow Management},
  journal = {Proceedings of the IEEE},
  year = {2008},
  volume = {96},
  pages = {2060 -2080},
  number = {12},
  month = {dec. },
  doi = {10.1109/JPROC.2008.2006141},
  issn = {0018-9219},
  keywords = {air traffic control;airport;decision theory;human factor;operations
	research;optimization;software engineering;traffic flow management;air
	traffic control;airports;decision theory;optimisation;}
}

@ARTICLE{664154,
  author = {Tomlin, C. and Pappas, G.J. and Sastry, S.},
  title = {Conflict resolution for air traffic management: a study in multiagent
	hybrid systems},
  journal = {Automatic Control, IEEE Transactions on},
  year = {1998},
  volume = {43},
  pages = {509 -521},
  number = {4},
  month = {apr},
  doi = {10.1109/9.664154},
  issn = {0018-9286},
  keywords = {ATC;air traffic management;aircraft control;distributed control system;multiagent
	hybrid systems;trajectory conflict resolution;verification;aerodynamics;air
	traffic control;aircraft control;cooperative systems;distributed
	control;}
}

@INPROCEEDINGS{Coordination,
  author = {N. Vlassis and R. Elhorst and J. R. Kok},
  title = {Anytime algorithms for multiagent decision making using coordination
	graphs},
  booktitle = {International Conference on Systems, Man and Cybernetics},
  year = {2004},
  owner = {Will},
  timestamp = {2012.10.10}
}

@ARTICLE{tumer-wolpert_jair02,
  author = {D. H. Wolpert and K. Tumer},
  title = {Collective Intelligence, Data Routing and {B}raess' Paradox},
  journal = {Journal of Artificial Intelligence Research},
  year = {2002},
  volume = {16},
  pages = {359-387},
  abstract = {We consider the problem of designing the the utility functions of
	the utility-maximizing agents in a multi-agent system (MAS) so that
	they work synergistically to maximize a global utility. The particular
	problem domain we explore is the control of network routing by placing
	agents on all the routers in the network. Conventional approaches
	to this task have the agents all use the Ideal Shortest Path routing
	Algorithm (ISPA). We demonstrate that in many cases, due to the side-effects
	of one agent's actions on another agent's performance, having agents
	use ISPA's is suboptimal as far as global aggregate cost is concerned,
	even when they are only used to route infinitesimally small amounts
	of traffic. The utility functions of the individual agents are not
	"aligned" with the global utility, intuitively speaking. As a particular
	example of this we present an instance of Braess' paradox in which
	adding new links to a network whose agents all use the ISPA results
	in a <em>decrease</em> in overall throughput. We also demonstrate
	that load-balancing, in which the agents' decisions are collectively
	made to optimize the global cost incurred by all traffic <em>currently</em>
	being routed, is suboptimal as far as global cost <em>averaged across
	time</em> is concerned. This is also due to "side-effects", in this
	case of current routing decision on future traffic. The mathematics
	of Collective Intelligence (COIN) is concerned precisely with the
	issue of avoiding such deleterious side-effects in multi-agent systems,
	both over time and space. We present key concepts from that mathematics
	and use them to derive an algorithm whose ideal version should have
	better performance than that of having all agents use the ISPA, even
	in the infinitesimal limit. We present experiments verifying this,
	and also showing that a machine-learning-based version of this COIN
	algorithm in which costs are only imprecisely estimated via empirical
	means (a version potentially applicable in the real world) also outperforms
	the ISPA, despite having access to less information than does the
	ISPA. In particular, this COIN algorithm almost always avoids Braess'
	paradox.},
  bib2html_pubtype = {Journal Articles},
  bib2html_rescat = {Multiagent Systems, Collectives, Traffic and Transportation}
}

