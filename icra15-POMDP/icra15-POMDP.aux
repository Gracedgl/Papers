\relax 
\citation{darpa}
\citation{6343870}
\citation{5980259}
\citation{Goodrich:2007:HIS:1348099.1348100}
\citation{5980259}
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}}
\citation{darpa}
\citation{5980259}
\citation{6343870}
\citation{goodfellow_help_2010}
\citation{Dragan_2013_7390}
\citation{6256067}
\citation{helicopter}
\citation{1639157}
\citation{Dragan_2013_7390}
\citation{Bruemmer:2005:SUC:2229264.2230046}
\citation{Sutton98reinforcementlearning}
\citation{Kalyanakrishnan:2009:EAV:1558109.1558115}
\citation{Sutton98reinforcementlearning}
\citation{Geibel:2005:RRL:1622519.1622522}
\citation{Borkar:2002:QRC:767822.769466}
\citation{Liu:2003:RAA:860575.860632}
\citation{Coraluppi1999301}
\@writefile{toc}{\contentsline {section}{\numberline {II}BACKGROUND}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-A}Robot Autonomy and Teleoperation}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-B}Reinforcement Learning}{2}}
\citation{smach}
\citation{288}
\citation{smach}
\citation{journals/nn/StringerRT07}
\citation{Watkins:1989}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Example SMACH state machine used to allow the PR2 robot to charge itself.}}{3}}
\newlabel{SMACH}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-C}SMACH}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {III}As Autonomous As Possible}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-A}$A^3P$ Overview}{3}}
\citation{pr2}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Implementing SMACH and $A^3P$ states concurrently is straight forward. In this example, we created a top level SMACH state machine, added a reinforcement learning state for navigation, and terminated.}}{4}}
\newlabel{CodeSnippet}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-B}Learning Implementation}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Validation}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Corridor Domain}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Corridor domain map.}}{4}}
\newlabel{Corridor}{{3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces SMACH state machine used to navigate through the corridor domain.}}{5}}
\newlabel{SMACH_Corridor}{{4}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces True mean, standard deviation and transition probabilities for our corridor domain.}}{5}}
\newlabel{IdealQTable}{{I}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The mean square error of the transition function, mean, and standard deviation over each navigation related state-action pairs.}}{5}}
\newlabel{LearnedQTable}{{II}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Results}{5}}
\bibstyle{plain}
\bibdata{thesis}
\bibcite{smach}{1}
\bibcite{Borkar:2002:QRC:767822.769466}{2}
\bibcite{Bruemmer:2005:SUC:2229264.2230046}{3}
\bibcite{Coraluppi1999301}{4}
\bibcite{darpa}{5}
\bibcite{Dragan_2013_7390}{6}
\bibcite{1639157}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The $A^3P$ Q-Table converged to the true number of states: 13 navigation (Table I\hbox {}), 2 task, and 2 initial (Figure 4\hbox {}). These results were found over 10 statistical runs and error bars are included.}}{6}}
\newlabel{State_Convergence}{{5}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The policy learned by $A^3P$ is an optimal risk-free policy. The policy learned by traditional Q-Learning results in undesirable outliers.}}{6}}
\newlabel{policy_hist}{{6}{6}}
\@writefile{toc}{\contentsline {section}{References}{6}}
\bibcite{6256067}{8}
\bibcite{pr2}{9}
\bibcite{Geibel:2005:RRL:1622519.1622522}{10}
\bibcite{goodfellow_help_2010}{11}
\bibcite{Goodrich:2007:HIS:1348099.1348100}{12}
\bibcite{Kalyanakrishnan:2009:EAV:1558109.1558115}{13}
\bibcite{Liu:2003:RAA:860575.860632}{14}
\bibcite{6343870}{15}
\bibcite{helicopter}{16}
\bibcite{5980259}{17}
\bibcite{288}{18}
\bibcite{journals/nn/StringerRT07}{19}
\bibcite{Sutton98reinforcementlearning}{20}
\bibcite{Watkins:1989}{21}
