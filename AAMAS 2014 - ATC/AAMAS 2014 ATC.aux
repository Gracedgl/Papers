\relax 
\citation{faa05}
\citation{faa05}
\citation{Bertsimas}
\citation{Rios}
\citation{tumer-agogino_jaamas12}
\citation{Sridhar}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{\thepage }}
\citation{716791}
\citation{Sun98someexperiments}
\citation{Dayan93feudalreinforcement}
\citation{Reddy_learninggoal-decomposition}
\citation{Bertsimas:1998:ATF:767667.768027}
\citation{Bilimoria}
\citation{McNally}
\citation{Mueller_analysisof}
\citation{Bilimoria}
\citation{McNally}
\citation{Bertsimas:1998:ATF:767667.768027}
\citation{Mueller_analysisof}
\citation{Bertsimas}
\citation{Agogino:2009:EEM:1570256.1570258}
\citation{Rios}
\citation{tumer-agogino_jaamas12}
\citation{Curran:2013:AHC:2484920.2485183}
\citation{664154}
\citation{Sislak:2008:AMA:1402744.1402755}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{\thepage }}
\newlabel{sec:BACKGROUND}{{2}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Agent Partitioning}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Air Traffic Flow Management Problem}{\thepage }}
\citation{tumer-wolpert_jair02}
\citation{tumer-agogino_jaamas12}
\citation{AAMAS12-agmon}
\citation{5509316}
\citation{Colby:2012:SFF:2343576.2343637}
\citation{Hardin}
\citation{tumer-wolpert_jair02}
\citation{AAMAS12-agmon}
\citation{Agogino:2012:ELS:2330163.2330306}
\citation{Colby:2012:SFF:2343576.2343637}
\citation{tumer-wolpert_jair02}
\citation{Junges:2008:EPD:1402298.1402308}
\citation{Modi:2005:AAD:1120120.1120127}
\citation{Rios}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Reward Shaping}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Constraint Optimization}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {3}ATFMP Approach}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Agent Definition}{\thepage }}
\citation{6095996}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Reward Structures}{\thepage }}
\newlabel{eq:Global}{{2}{\thepage }}
\newlabel{eq:Cumilative Delay}{{3}{\thepage }}
\newlabel{eq:Cumilative Congestion}{{4}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Soft Constraint Application}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Hard Constraint Application}{\thepage }}
\citation{journals/advcs/AgoginoT09}
\citation{6095996}
\citation{Agglomerative}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Delay and congestion with varying $w$, the weight on congestion. No matter the weight, the learning algorithm will not remove congestion while attempting to keep delay low. Additionally the cost to delay for when removing congestion is not a 1:1 mapping. A removal of 10,000 congestion adds 70,000 minutes of delay}}{\thepage }}
\newlabel{delayCongestionPower}{{1}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Evaluation using the greedy scheduler}}{\thepage }}
\newlabel{LearningCycle}{{2}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Agent Partitioning}{\thepage }}
\citation{FACET}
\citation{Sislak:2008:AMA:1402744.1402755}
\@writefile{toc}{\contentsline {section}{\numberline {4}Simulator Characteristics}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Learning with Hard Constraints}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The highest level of partitioning for the system-level reward (G) is displayed here and compared with zero partitioning and the greedy scheduler. The system-level reward performed worse with fewer partitions, but still better than zero partitioning. Even though partitioning performed better than zero partitioning, the system-level reward could not come close to beating the greedy scheduler. Using this learning approach, the system-level reward performance could never become better than the greedy scheduler.}}{\thepage }}
\newlabel{ClusterRewardsGreedyG}{{3}{\thepage }}
\bibstyle{plain}
\bibdata{thesis}
\bibcite{AAMAS12-agmon}{1}
\bibcite{6095996}{2}
\bibcite{tumer-agogino_jaamas12}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A closer look at the difference reward performance using the smaller number of partitions shows a 37\% improvement over the greedy scheduling solution.}}{\thepage }}
\newlabel{ATFMPOldDvsGreedy}{{4}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Partition Comparison for Hard Constraints}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces With the greater number of partitions, the learning performance decreases and speed increases. Note that the outlier is a artifact of the greedy scheduler discussed in this section.}}{\thepage }}
\newlabel{ATFMPOldTable}{{1}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{\thepage }}
\newlabel{sec:CONCLUSION}{{6}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {7}References}{\thepage }}
\bibcite{Agogino:2009:EEM:1570256.1570258}{4}
\bibcite{Agogino:2012:ELS:2330163.2330306}{5}
\bibcite{journals/advcs/AgoginoT09}{6}
\bibcite{Bertsimas}{7}
\bibcite{Bertsimas:1998:ATF:767667.768027}{8}
\bibcite{FACET}{9}
\bibcite{Bilimoria}{10}
\bibcite{tumer-colby_gecco11}{11}
\bibcite{Colby:2012:SFF:2343576.2343637}{12}
\bibcite{Curran:2013:AHC:2484920.2485183}{13}
\bibcite{faa05}{14}
\bibcite{Agglomerative}{15}
\bibcite{Dayan93feudalreinforcement}{16}
\bibcite{Hardin}{17}
\bibcite{716791}{18}
\bibcite{Junges:2008:EPD:1402298.1402308}{19}
\bibcite{5509316}{20}
\bibcite{McNally}{21}
\bibcite{Modi:2005:AAD:1120120.1120127}{22}
\bibcite{Mueller_analysisof}{23}
\bibcite{Reddy_learninggoal-decomposition}{24}
\bibcite{Rios}{25}
\bibcite{Sridhar}{26}
\bibcite{Sun98someexperiments}{27}
\bibcite{664154}{28}
\bibcite{Sislak:2008:AMA:1402744.1402755}{29}
\bibcite{tumer-wolpert_jair02}{30}
