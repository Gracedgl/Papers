Thank you all for your thoughtful reviews. 

The domain-based simulation results were not included in this submission due to space constraints. In the prior submission, a comparison and analysis were included comparing the domain-based partitioning in the ATFMP to our RUBI partitioning. The key takeaway from this comparison is that RUBI-based partitioning led to a reward-independent partitioning involving 61 partitions, but in domain-based partitioning the smallest was 3. This leads to much faster simulation time at no cost to performance and is noted on page 6 of the AAAI-15 submission. In the AAAI-14 submission it was noted that RUBI was successfully applied to the El Farol Bar Problem, but few results and no analysis was given. Many reviewers asked for more information about this experiment, so the domain-based comparison was removed to make space for the El Farol Bar Problem. Since this has been mentioned now by many reviewers, the camera-ready copy would include the domain-based comparison originally excluded from the paper as well as the El Farol Bar Problem results, and we would pay for an additional page. 

RUBI uses the local reward returned by the simulation to partition agents. Agents do not learn in this process. It is a pre-processing technique to discover the underlying partitioning of the system. This is simple and fast, as it is meant to be applied to extremely large multiagent systems (in this case ~40,000). RUBI also finds multiple levels of partitioning, with differing sizes of partitions. Each partition is treated independently of each other partition whether or not they are actually independent (this is what was meant in the paper by 'agents are considered reward independent'). This leads to a cost-benefit tradeoff of performance and speed. With the smallest number of partitions, partitions are guaranteed to be independent, leading to the best performance at the slowest speed. In the ATFMP this resulted in a 37% increase in performance over the greedy scheduler, with a 510x speed up over non-partitioning approaches. With a larger number of partitions the speed of simulation greatly increases, at the cost of performance. This resulted in a 20% increase in performance over the greedy scheduler, with a 5400x speed up over non-partitioning approaches. All levels of partitioning explored are computationally tractable and perform better than the current real-world ATFMP approach. 

It cannot be assumed that if an approach leads to better performance with a small number of agents, then that approach will work well for a larger number of agents (especially orders of magnitude more). Some examples include DCOPs, the work by Zhang and Lesser, and Factored MDPs. These approaches do not scale to extremely large domains. Since RUBI partitioning is performed as a pre-processing technique, it benefits from not slowing down the actual on-line learning process. In the ATFMP, learning the underlying partitioning on-line leads to intractability.

We experimentally validate our claims that RUBI performs an accurate partitioning in the El Farol Bar Problem and the ATFMP. In the ATFMP a tractable upper bound would be to assume no congestion in the system. This is unrealistic and trivial and so is not provided for comparison. Instead, the we compare to the greedy scheduler solution, the current real-world solution used by the FAA. We use this as our baseline, as it is considered to be a viable solution. Calculating a realistic upper bound with congestion would be an exhaustive search across all agent action combinations.

Testing RUBI in a stateful system is reserved for future work. In the ATFMP, there are 40,000 agents, each with 11 actions. This leads to 440,000 joint-actions. It can be said that RUBI will then work in a domain with at most 440,000 joint state-action pairs. This will be experimentally validated in future work, as well as be extended to a larger domain with more state-action pairs. In the camera-ready copy we will include this future work statement to address reviewer interest.
