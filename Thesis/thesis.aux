\relax 
\citation{faa05}
\citation{faa05}
\citation{Bertsimas}
\citation{Rios}
\citation{tumer-agogino_jaamas12}
\citation{Sridhar}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{Sutton98reinforcementlearning}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Multiagent Systems}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Reinforcement Learning}{5}}
\citation{Sutton98reinforcementlearning}
\citation{Sutton98reinforcementlearning}
\citation{tuyls2006learning}
\newlabel{eq:Action Value Learning Update}{{2.1}{6}}
\newlabel{alg:Action Value Learning Algorithm}{{2.1.1}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Action-Value Learning}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Multiagent Learning}{7}}
\citation{tumer-wolpert_jair02}
\citation{tumer-agogino_jaamas12}
\citation{Sislak:2008:AMA:1402744.1402755}
\citation{AAMAS12-agmon}
\citation{5509316}
\citation{Colby:2012:SFF:2343576.2343637}
\citation{Hardin}
\citation{tumer-wolpert_jair02}
\citation{AAMAS12-agmon}
\citation{tumer-agogino_jaamas08}
\citation{Agogino:2012:ELS:2330163.2330306}
\citation{Colby:2012:SFF:2343576.2343637}
\citation{Sislak:2008:AMA:1402744.1402755}
\citation{tumer-wolpert_jair02}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Reward Shaping for Coordination}{8}}
\citation{Nwana96co-ordinationin}
\citation{Zhang-406}
\citation{Dietterich00hierarchicalreinforcement}
\citation{Doucette:2012:HTD:2330163.2330178}
\citation{4708962}
\citation{399902}
\citation{788664}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Agent Coordination}{9}}
\citation{Guttmann_makingallocations}
\citation{Guestrin:2002:CRL:645531.757784}
\citation{Knudson:2010:RCA:1838206.1838422}
\citation{Guestrin:2002:CRL:645531.757784}
\citation{Tambe97towardsflexible}
\citation{Jain:2010:DCY:1755267.1755654}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Agent Partitioning}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Clustering algorithms}{10}}
\citation{Zhao:2002:EHC:584792.584877}
\citation{Bottegoni15072006}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Hierarchical Agglomerative Clustering}{11}}
\citation{Bertsimas}
\citation{Agogino:2009:EEM:1570256.1570258}
\citation{Rios}
\citation{tumer-agogino_jaamas12}
\citation{Curran:2013:AHC:2484920.2485183}
\citation{664154}
\citation{Sislak:2008:AMA:1402744.1402755}
\citation{Zhang95areinforcement}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Hierarchical Agglomerative Clustering}}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Related Work}{12}}
\citation{Junges:2008:EPD:1402298.1402308}
\citation{Modi:2005:AAD:1120120.1120127}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Air Traffic Flow Management Problem}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Constraint Optimization}{13}}
\citation{Rios}
\citation{716791}
\citation{Sun98someexperiments}
\citation{Dayan93feudalreinforcement}
\citation{Reddy_learninggoal-decomposition}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Agent Partitioning}{14}}
\citation{BarProblem}
\citation{BarProblem}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Domains}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Heterogeneous Bar Problem}{15}}
\newlabel{eq:BarProblem-Local}{{2.3}{16}}
\newlabel{eq:BarProblem-Global}{{2.4}{16}}
\citation{Bertsimas:1998:ATF:767667.768027}
\citation{Bilimoria}
\citation{McNally}
\citation{Mueller_analysisof}
\citation{Bilimoria}
\citation{McNally}
\citation{Bertsimas:1998:ATF:767667.768027}
\citation{Mueller_analysisof}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Air Traffic Flow Management Problem}{17}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Approach}{18}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Agent Definition}{18}}
\citation{Sutton98reinforcementlearning}
\citation{holmes-aamas}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Agent Learning}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Reward Structures}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Assigning the System-Level reward}{21}}
\newlabel{eq:Global}{{3.2}{21}}
\newlabel{eq:Cumilative Delay}{{3.3}{21}}
\newlabel{eq:Cumilative Congestion}{{3.4}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Applying the Difference Reward}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Soft Constraint Application}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Delay and congestion with varying $w$, the weight on congestion. No matter the weight, the learning algorithm will not remove congestion while attempting to keep delay low. Additionally the cost to delay for when removing congestion is not a 1:1 mapping. A removal of 10,000 congestion adds 70,000 minutes of delay}}{24}}
\newlabel{delayCongestionPower}{{3.1}{24}}
\citation{6095996}
\citation{journals/advcs/AgoginoT09}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Hard Constraint Optimization}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Evaluation using the greedy scheduler}}{26}}
\newlabel{LearningCycle}{{3.2}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Agent Partitioning}{26}}
\citation{tumer-colby_gecco11}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Generic Coupled Learning System using Difference Reward}}{28}}
\newlabel{alg:Coupled}{{3}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Computational Complexity}{28}}
\citation{FACET}
\citation{Sislak:2008:AMA:1402744.1402755}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Simulator Characteristics}{29}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Greedy Scheduler}}{30}}
\citation{6095996}
\citation{Agglomerative}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Partitioning Agents}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Domain-Based Partitioning}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Reward/Utility Based Impact Algorithm}{32}}
\newlabel{eq:RUBI Update}{{4.3}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Implementation of RUBI}{33}}
\newlabel{alg:RUBI}{{4.2.1}{34}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Reward/Utility Based Impact Algorithm}}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Impact Calculation}{34}}
\newlabel{eq:RUBI ATFMP-L}{{4.5}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Simulation}{35}}
\citation{6378284}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Computational Cost}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Benefits of RUBI}{37}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experimental Results}{39}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Hard Constraint Optimization with Domain-Based Partitioning}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Learning with Soft Constraints}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Learning with Hard Constraints}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Using the system-level performance as a reward does not work well in this large multiagent system. The difference reward was able to lower congestion much more, but could not manage to also reduce delay. Note that these are best performing experiments for the difference reward and system-level reward.}}{42}}
\newlabel{NonClusterVsCluster}{{5.1}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The highest level of partitioning for the system-level reward (G) is displayed here and compared with zero partitioning and the greedy scheduler. The system-level reward performed worse with fewer partitions, but still better than zero partitioning. Even though partitioning performed better than zero partitioning, the system-level reward could not come close to beating the greedy scheduler. Using this learning approach, the system-level reward performance could never become better than the greedy scheduler.}}{44}}
\newlabel{ClusterRewardsGreedyG}{{5.2}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces A closer look at the difference reward performance using the smaller number of partitions shows a 37\% improvement over the greedy scheduling solution.}}{45}}
\newlabel{ATFMPOldDvsGreedy}{{5.3}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Partition Comparisons for Hard Constraints}{46}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces With the greater number of partitions, the learning performance decreases and speed increases. Note that the outlier is a artifact of the greedy scheduler discussed in this section.}}{47}}
\newlabel{ATFMPOldTable}{{5.1}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Hard Constraint Optimization using RUBI}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Heterogeneous Bar Problem}{47}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces With the greater number of partitions, the learning performance decreases and speed increases.}}{48}}
\newlabel{BarProblemTable}{{5.2}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces As the number of partitions decrease, the agents receive less information about the environment, and performance decreases. In this domain, 2 and 3 partitions work equally well as the difference reward, but with 6x faster simulation rate.}}{49}}
\newlabel{BarProblemNewPartitions}{{5.4}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces This graph represents the scaled value of different performance metrics. For example, a scaled value of .50 for the converged performance represents that this is 50\% of the best converged performance. As self-similarity decreases, the performance and average time taken per learning step decreases. This trend rate begins slow, but increases dramatically once self-similarity is less than 67\%. In this domain, this level of self-similarity is an important metric to stay above while partitioning.}}{50}}
\newlabel{BarProblemComparison}{{5.5}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}ATFMP}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2.1}RUBI Performance in the ATFMP}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces As the number of partitions decreases, performance improves while time complexity increases. Note that a reward independent partitioning using RUBI includes 61 partitions. }}{52}}
\newlabel{ATFMPNewvsGreedy}{{5.6}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Comparison Between RUBI and Domain-Based Partitioning}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces As the self-similarity increases, final performance and time taken per learning step increases. Note that final performance is a 6th degree polynomial trend line with $R^2 = .95$}}{54}}
\newlabel{NewPartitionComparisons}{{5.7}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Partitions formed with RUBI had higher self-similarity than using domain-based partitioning. This leads to higher quality learning with respect to each partition.}}{55}}
\newlabel{Self-SimOldvsSelf-SimNew}{{5.8}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces The final performance of RUBI partitions compared to domain-based partitions. Initially domain-based partitions perform better than RUBI, with RUBI performing better with a smaller number of partitions. We see that this is not a good performance metric, as it does not take into account individual partition sizes. Note that the a 6th degree polynomial was used in creating this graph with an $R^2$ value of $.95$ for partitioning with RUBI and $.92$ for domain-based partitioning.}}{58}}
\newlabel{FinalOldvsFinalNew}{{5.9}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Initially, domain-based partitioning has a larger average size of partitions, leading to a higher initial performance. With a smaller number of partitions, RUBI partitioning had a larger size of partitions. Note that when all partitions are reward independent, domain-based partitioning had only a few number of very large partitions. RUBI partitioning on the other hand included a much smaller average size, and many more partitions. }}{59}}
\newlabel{OldvsNewAvgSize}{{5.10}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces When comparing individual partition size averages to performance, RUBI partitions perform much better than domain specific partitions with respect to average partition size and final performance.}}{60}}
\newlabel{ATFMPPerformancevsAvgSize}{{5.11}{60}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{61}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{Panait}
\citation{Kok:2006:CMR:1248547.1248612}
\citation{Coordination}
\bibstyle{plain}
\bibdata{thesis}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Future Work}{62}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{62}}
\bibcite{AAMAS12-agmon}{1}
\bibcite{6095996}{2}
\bibcite{tumer-agogino_jaamas08}{3}
\bibcite{tumer-agogino_jaamas12}{4}
\bibcite{Agogino:2009:EEM:1570256.1570258}{5}
\bibcite{Agogino:2012:ELS:2330163.2330306}{6}
\bibcite{journals/advcs/AgoginoT09}{7}
\bibcite{BarProblem}{8}
\bibcite{Bertsimas}{9}
\bibcite{Bertsimas:1998:ATF:767667.768027}{10}
\bibcite{FACET}{11}
\bibcite{Bilimoria}{12}
\bibcite{Bottegoni15072006}{13}
\bibcite{tumer-colby_gecco11}{14}
\bibcite{Colby:2012:SFF:2343576.2343637}{15}
\bibcite{Curran:2013:AHC:2484920.2485183}{16}
\bibcite{faa05}{17}
\bibcite{Agglomerative}{18}
\bibcite{Dayan93feudalreinforcement}{19}
\bibcite{Dietterich00hierarchicalreinforcement}{20}
\bibcite{Doucette:2012:HTD:2330163.2330178}{21}
\bibcite{Guttmann_makingallocations}{22}
\bibcite{Hardin}{23}
\bibcite{Guestrin:2002:CRL:645531.757784}{24}
\bibcite{holmes-aamas}{25}
\bibcite{Jain:2010:DCY:1755267.1755654}{26}
\bibcite{716791}{27}
\bibcite{Junges:2008:EPD:1402298.1402308}{28}
\bibcite{5509316}{29}
\bibcite{Knudson:2010:RCA:1838206.1838422}{30}
\bibcite{Kok:2006:CMR:1248547.1248612}{31}
\bibcite{Panait}{32}
\bibcite{788664}{33}
\bibcite{McNally}{34}
\bibcite{Modi:2005:AAD:1120120.1120127}{35}
\bibcite{Mueller_analysisof}{36}
\bibcite{Nwana96co-ordinationin}{37}
\bibcite{Reddy_learninggoal-decomposition}{38}
\bibcite{Rios}{39}
\bibcite{Sridhar}{40}
\bibcite{Sun98someexperiments}{41}
\bibcite{Sutton98reinforcementlearning}{42}
\bibcite{Tambe97towardsflexible}{43}
\bibcite{664154}{44}
\bibcite{tuyls2006learning}{45}
\bibcite{399902}{46}
\bibcite{Coordination}{47}
\bibcite{Sislak:2008:AMA:1402744.1402755}{48}
\bibcite{tumer-wolpert_jair02}{49}
\bibcite{4708962}{50}
\bibcite{6378284}{51}
\bibcite{Zhang-406}{52}
\bibcite{Zhang95areinforcement}{53}
\bibcite{Zhao:2002:EHC:584792.584877}{54}
